\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{main.ist}
\@glsorder{word}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Conventions}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}QCD}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}lattice QCD}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Performance Models}{2}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Software: openQxD}{2}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conjugate Gradient algorithm}{2}{section.7}\protected@file@percent }
\newlabel{eq:Axb}{{7.1}{2}{Conjugate Gradient algorithm}{equation.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Quandratic form TODO}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:qform}{{1}{3}{Quandratic form TODO}{figure.1}{}}
\newlabel{eq:fp_cgne}{{{7.2}}{3}{Conjugate Gradient algorithm}{AMS.3}{}}
\newlabel{eq:error}{{7.3a}{4}{Error and Residual}{equation.7.3a}{}}
\newlabel{eq:residual}{{7.3b}{4}{Error and Residual}{equation.7.3b}{}}
\newlabel{eq:steepest_descent}{{7.4}{4}{Method of Steepest Descent}{equation.7.4}{}}
\newlabel{eq:cg_step}{{7.5}{4}{Conjugate Gradient algorithm}{equation.7.5}{}}
\newlabel{eq:cg_error1}{{7.6b}{4}{Conjugate Gradient algorithm}{equation.7.6b}{}}
\newlabel{eq:cg_error2}{{7.6c}{5}{Conjugate Gradient algorithm}{equation.7.6c}{}}
\newlabel{eq:residual_exact}{{7.7a}{5}{Conjugate Gradient algorithm}{equation.7.7a}{}}
\newlabel{eq:residual_recursive}{{7.7b}{5}{Conjugate Gradient algorithm}{equation.7.7b}{}}
\newlabel{eq:cg_error2}{{7.7c}{5}{Conjugate Gradient algorithm}{equation.7.7c}{}}
\newlabel{eq:error_i}{{{7.8}}{5}{Conjugate Gradient algorithm}{AMS.5}{}}
\newlabel{eq:cgne:alpha_pre}{{7.9}{6}{Conjugate Gradient algorithm}{equation.7.9}{}}
\newlabel{df:gramschmidt}{{7.4}{6}{Gram-Schmidt Orthogonalisation}{definition.7.4}{}}
\newlabel{eq:gramschmidt}{{7.10}{6}{Gram-Schmidt Orthogonalisation}{equation.7.10}{}}
\newlabel{eq:betas}{{7.11}{7}{Conjugate Gradient algorithm}{equation.7.11}{}}
\newlabel{eq:uiAdj}{{{7.12}}{7}{Conjugate Gradient algorithm}{AMS.7}{}}
\newlabel{lem:rorthogonality}{{7.2}{8}{}{theorem.7.2}{}}
\citation{openqxd}
\citation{shewchuk1994}
\newlabel{eq:alphai}{{7.13}{9}{Method of conjugate gradient}{equation.7.13}{}}
\newlabel{eq:betai}{{7.14}{9}{Method of conjugate gradient}{equation.7.14}{}}
\citation{openqxd}
\citation{ieee754_1985}
\gincltex@bb{schemes/float.tex}{0}{0}{501.02545}{54.19151}
\@writefile{toc}{\contentsline {section}{\numberline {8}Real number formats}{10}{section.8}\protected@file@percent }
\newlabel{sec:floats}{{8}{10}{Real number formats}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}IEEE Standard for Floating-Point Arithmetic}{10}{subsection.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Binary representation of a IEEE 754 $n$-bit precision floating-point number. The \leavevmode {\color {corange}orange} bit represents the \leavevmode {\color {corange}sign bit}, the \leavevmode {\color {cblue}blue} bits represent the fixed-length \leavevmode {\color {cblue}e exponent bits} and the \leavevmode {\color {cgreen}green} bits represent the fixed-length \leavevmode {\color {cgreen}m mantissa bits}. Notice that $n = \leavevmode {\color {corange}1} + \leavevmode {\color {cblue}e} + \leavevmode {\color {cgreen}m}$.}}{10}{figure.2}\protected@file@percent }
\newlabel{fig:float}{{2}{10}{Binary representation of a IEEE 754 $n$-bit precision floating-point number. The \textcolor {corange}{orange} bit represents the \textcolor {corange}{sign bit}, the \textcolor {cblue}{blue} bits represent the fixed-length \textcolor {cblue}{e exponent bits} and the \textcolor {cgreen}{green} bits represent the fixed-length \textcolor {cgreen}{m mantissa bits}. Notice that $n = \textcolor {corange}{1} + \textcolor {cblue}{e} + \textcolor {cgreen}{m}$}{figure.2}{}}
\citation{ieee754_2008}
\citation{ieee754_2008}
\citation{ieee754_2008}
\citation{bfloat16}
\citation{tf32}
\citation{fp24}
\citation{ieee754_2008}
\citation{ieee754_2008}
\citation{ieee754_1985}
\newlabel{eq:exponent}{{8.1}{11}{IEEE Standard for Floating-Point Arithmetic}{equation.8.1}{}}
\savepointas{implicit1}{pgfid3}{0pt}{0pt}
\savepicturepage{pgfid3}{11}
\pgfsyspdfmark {pgfid3}{18021591}{31201117}
\savepicturepage{pgfid4}{11}
\pgfsyspdfmark {pgfid4}{6390611}{29665127}
\savepointas{implicit0}{pgfid6}{0pt}{0pt}
\savepicturepage{pgfid6}{11}
\pgfsyspdfmark {pgfid6}{18021591}{26312788}
\savepicturepage{pgfid7}{11}
\pgfsyspdfmark {pgfid7}{6390611}{24776798}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Commonly used floating point formats, where $s$ is the number of sign bits, $e$ the number of exponent bits and $m$ the number of mantissa bits.}}{12}{table.1}\protected@file@percent }
\newlabel{tab:formats}{{1}{12}{Commonly used floating point formats, where $s$ is the number of sign bits, $e$ the number of exponent bits and $m$ the number of mantissa bits}{table.1}{}}
\citation{cody1980}
\citation{gustafson2017}
\gincltex@bb{schemes/posit.tex}{0}{0}{501.30443}{54.19151}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Summary of highest representable numbers, minimal subnormal and non-subnormal representable numbers above \num {0} in any IEEE 754 floating point format together with their approximated precision.}}{13}{table.2}\protected@file@percent }
\newlabel{tab:float:limits}{{2}{13}{Summary of highest representable numbers, minimal subnormal and non-subnormal representable numbers above \num {0} in any IEEE 754 floating point format together with their approximated precision}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Posits}{13}{subsection.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Binary representation of a $n$-bit posit number. As with regular floats the \leavevmode {\color {corange}orange} bit represents the \leavevmode {\color {corange}sign bit}, the \leavevmode {\color {cyellow}yellow} bit(s) represent the variable length \leavevmode {\color {cyellow}regime bit(s)} terminated by the \leavevmode {\color {cbrown}brown} bit that is the \leavevmode {\color {cbrown}opposite regime bit}, the \leavevmode {\color {cblue}blue} bit(s) represent the variable-length \leavevmode {\color {cblue}exponent bit(s)} and the \leavevmode {\color {cgreen}green} bit(s) represent the variable-length \leavevmode {\color {cgreen}mantissa bit(s)}.}}{13}{figure.3}\protected@file@percent }
\newlabel{fig:posit}{{3}{13}{Binary representation of a $n$-bit posit number. As with regular floats the \textcolor {corange}{orange} bit represents the \textcolor {corange}{sign bit}, the \textcolor {cyellow}{yellow} bit(s) represent the variable length \textcolor {cyellow}{regime bit(s)} terminated by the \textcolor {cbrown}{brown} bit that is the \textcolor {cbrown}{opposite regime bit}, the \textcolor {cblue}{blue} bit(s) represent the variable-length \textcolor {cblue}{exponent bit(s)} and the \textcolor {cgreen}{green} bit(s) represent the variable-length \textcolor {cgreen}{mantissa bit(s)}}{figure.3}{}}
\citation{ieee754_2008}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Summary of highest representable numbers, minimal representable numbers above \num {0} in any posit format together with their approximated precision.}}{15}{table.3}\protected@file@percent }
\newlabel{tab:posit:limits}{{3}{15}{Summary of highest representable numbers, minimal representable numbers above \num {0} in any posit format together with their approximated precision}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Density or distribution of numbers for \gls {tensorfloat32}, \gls {binary16}, \gls {posit16} and \gls {bfloat16}. The number of bins was chosen to be \num {1024} of logarithmic width. The IEEE conformant floats \gls {tensorfloat32}, \gls {binary16} and \gls {bfloat16} exhibit a simmilar shape, namely the distribution of numbers is exponential decreasing for higher and smaller numbers. The high numbers undergo a rough cutoff at the highest representable number. Numbers above that value will be cast to infinity. Compared to this, the small numbers show a smooth cutoff, because of the existence subnormal numbers. The range of \gls {posit16} is bigger than the range of \gls {binary16}, but specially in the very small numbers this difference in range is neglectible. Some features of posits can be observed: First, their distribution is symmetric around \num {1}, because posits have no subnormals. Second, more numbers are closer to \num {1} than in case of floats; the closer to \num {1}, the better the number resolution. Closest to \num {1}, the number resolution becomes better than \gls {binary16} resolution. Third, posits have no fixed-length mantissa nor exponent. That's the reason why the height of the posit shape depends on the number regime, which happens for floats only in the subnormal regime, where the exponent and mantissa are indeed of variable length. For all formats, the amount of numbers decreases exponentially when going away from \num {1}, but posits decrease faster. This suggests that when calculating in the number regime close to \num {1} posits might be the better choice, but when numbers span the whole number range equally, floats might be superior. But in that case one has to take care about over- and underflows. Notice that the height of the shape is determined by the number of mantissa bits, therefore giving the precision, whereas the width is determined by the number of exponent bits, therefore giving the number range. For example \gls {tensorfloat32} and \gls {binary16} have a very different number range, but exhibit the same percision for numbers in their intersection, meaning that \gls {binary16} is a subset of \gls {tensorfloat32}. On the other hand comparing \gls {tensorfloat32} and \gls {bfloat16} they have approximately the same number range, but different precisions in them, meaning that \gls {bfloat16} is as well a subset of \gls {tensorfloat32}, which itself is a subset of \gls {binary32}. Notice that when plotting \gls {binary32} and \gls {posit32} in such a plot, they would look very similar to \gls {binary16} versus \gls {posit16}.}}{15}{figure.4}\protected@file@percent }
\newlabel{fig:number_line}{{4}{15}{Density or distribution of numbers for \gls {tensorfloat32}, \gls {binary16}, \gls {posit16} and \gls {bfloat16}. The number of bins was chosen to be \num {1024} of logarithmic width. The IEEE conformant floats \gls {tensorfloat32}, \gls {binary16} and \gls {bfloat16} exhibit a simmilar shape, namely the distribution of numbers is exponential decreasing for higher and smaller numbers. The high numbers undergo a rough cutoff at the highest representable number. Numbers above that value will be cast to infinity. Compared to this, the small numbers show a smooth cutoff, because of the existence subnormal numbers. The range of \gls {posit16} is bigger than the range of \gls {binary16}, but specially in the very small numbers this difference in range is neglectible. Some features of posits can be observed: First, their distribution is symmetric around \num {1}, because posits have no subnormals. Second, more numbers are closer to \num {1} than in case of floats; the closer to \num {1}, the better the number resolution. Closest to \num {1}, the number resolution becomes better than \gls {binary16} resolution. Third, posits have no fixed-length mantissa nor exponent. That's the reason why the height of the posit shape depends on the number regime, which happens for floats only in the subnormal regime, where the exponent and mantissa are indeed of variable length. For all formats, the amount of numbers decreases exponentially when going away from \num {1}, but posits decrease faster. This suggests that when calculating in the number regime close to \num {1} posits might be the better choice, but when numbers span the whole number range equally, floats might be superior. But in that case one has to take care about over- and underflows. Notice that the height of the shape is determined by the number of mantissa bits, therefore giving the precision, whereas the width is determined by the number of exponent bits, therefore giving the number range. For example \gls {tensorfloat32} and \gls {binary16} have a very different number range, but exhibit the same percision for numbers in their intersection, meaning that \gls {binary16} is a subset of \gls {tensorfloat32}. On the other hand comparing \gls {tensorfloat32} and \gls {bfloat16} they have approximately the same number range, but different precisions in them, meaning that \gls {bfloat16} is as well a subset of \gls {tensorfloat32}, which itself is a subset of \gls {binary32}. Notice that when plotting \gls {binary32} and \gls {posit32} in such a plot, they would look very similar to \gls {binary16} versus \gls {posit16}}{figure.4}{}}
\citation{openqxd}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Exponent distribution of \gls {binary32} single precision floats in the residual vectors of all steps in a conjugate gradient run in openQxD as well as entries of the Dirac operator. 4 runs were made, with a lattice size of $4^4$ and $8^4$ on one single rank and $4$ ranks respectively. The number is normalised to $(-1)^s \cdot M \cdot 2^{E}$, where $M \in [1, 2)$.}}{16}{figure.5}\protected@file@percent }
\newlabel{fig:exponents}{{5}{16}{Exponent distribution of \gls {binary32} single precision floats in the residual vectors of all steps in a conjugate gradient run in openQxD as well as entries of the Dirac operator. 4 runs were made, with a lattice size of $4^4$ and $8^4$ on one single rank and $4$ ranks respectively. The number is normalised to $(-1)^s \cdot M \cdot 2^{E}$, where $M \in [1, 2)$}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Floating point numbers in openQxD}{16}{subsection.8.3}\protected@file@percent }
\newlabel{lst:cgne}{{1}{17}{The conjugate gradient kernel in \code {modules/linsolv/cgne.c} line \num {429}ff}{lstlisting.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}The conjugate gradient kernel in \texttt  {modules/linsolv/cgne.c} line \num {429}ff.}{17}{lstlisting.1}\protected@file@percent }
\newlabel{lst:break}{{2}{17}{break condition in \code {modules/linsolv/cgne.c} line \num {490}ff, \code {rn} is the norm of the current residual, \code {xn} is the norm of the current solution vector, both in \gls {binary32}}{lstlisting.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}break condition in \texttt  {modules/linsolv/cgne.c} line \num {490}ff, \texttt  {rn} is the norm of the current residual, \texttt  {xn} is the norm of the current solution vector, both in \gls {binary32}.}{17}{lstlisting.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}The conjugate gradient kernel}{17}{subsection.8.4}\protected@file@percent }
\citation{openqxd}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Simulating other datatypes}{18}{subsection.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.1}Discussion of figures \ref  {fig:cgne:naive} - \ref  {fig:cgne:res12}}{18}{subsubsection.8.5.1}\protected@file@percent }
\newlabel{eq:hierarchy}{{8.2}{18}{Discussion of figures \ref {fig:cgne:naive} - \ref {fig:cgne:res12}}{equation.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Convergence analysis of a conjugate gradient run, where \gls {binary32} was replaced by one of the simulated datatypes. The number \texttt  {s} describes the number of normal steps needed (the value of \texttt  {status}), whereas the numbers in the brackets indicate the number of reset steps. All reset steps are indicated by ticks at the dashed black line denoting the tolerance limit. The iterations will always go up to \texttt  {nmx=256}, but the range \num {80}-\num {256} is compressed since the most interesting behavior happens before step \num {80} for most of the simulated datatypes. The 6 plots show the naive replacement of the \gls {binary32} datatype with the simulated one. This means that every single variable containing a \gls {binary32} was replaced with a variable of the simulated datatype. Plot \textit  {1a} shows the exact residue \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:residual_exact}\unskip \@@italiccorr )}} calculated in every iteration using the Dirac matrix and the source vector both in \gls {binary64}, whereas plot \textit  {1b} shows the norm of the recursively calculated residue \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:residual_recursive}\unskip \@@italiccorr )}} (casted from the simulated datatype to \gls {binary64}). The relative residue suffers roundoff accumulation because of the recusive calculation; this is the differnce between plots \textit  {1a} and \textit  {1b}, which is plotted in plot \textit  {1c}. Plot \textit  {1d} shows the $A$-orthogonality of the current direction to the last direction, namely the value of $\vec  {p}_{i}^\dagger A \vec  {p}_{i+1}$. The last 2 plots, \textit  {1e} and \textit  {1f}, show the values of the amounts $\alpha _i$ and $\beta _i$ (see equations \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:alphai}\unskip \@@italiccorr )}} and \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:betai}\unskip \@@italiccorr )}}) in every iteration, but only of the datatypes that converged (\texttt  {status>0}). The lines in plot \textit  {1e} are linearly fitted to the data points ($f(x) = m x + b$). The number range of the slope $m$ is given in the plot legend.}}{19}{figure.6}\protected@file@percent }
\newlabel{fig:cgne:naive}{{6}{19}{Convergence analysis of a conjugate gradient run, where \gls {binary32} was replaced by one of the simulated datatypes. The number \code {s} describes the number of normal steps needed (the value of \code {status}), whereas the numbers in the brackets indicate the number of reset steps. All reset steps are indicated by ticks at the dashed black line denoting the tolerance limit. The iterations will always go up to \code {nmx=256}, but the range \num {80}-\num {256} is compressed since the most interesting behavior happens before step \num {80} for most of the simulated datatypes. The 6 plots show the naive replacement of the \gls {binary32} datatype with the simulated one. This means that every single variable containing a \gls {binary32} was replaced with a variable of the simulated datatype. Plot \textit {1a} shows the exact residue \eqref {eq:residual_exact} calculated in every iteration using the Dirac matrix and the source vector both in \gls {binary64}, whereas plot \textit {1b} shows the norm of the recursively calculated residue \eqref {eq:residual_recursive} (casted from the simulated datatype to \gls {binary64}). The relative residue suffers roundoff accumulation because of the recusive calculation; this is the differnce between plots \textit {1a} and \textit {1b}, which is plotted in plot \textit {1c}. Plot \textit {1d} shows the $A$-orthogonality of the current direction to the last direction, namely the value of $\vec {p}_{i}^\dagger A \vec {p}_{i+1}$. The last 2 plots, \textit {1e} and \textit {1f}, show the values of the amounts $\alpha _i$ and $\beta _i$ (see equations \eqref {eq:alphai} and \eqref {eq:betai}) in every iteration, but only of the datatypes that converged (\code {status>0}). The lines in plot \textit {1e} are linearly fitted to the data points ($f(x) = m x + b$). The number range of the slope $m$ is given in the plot legend}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces In these plots, the posits were utilizing \glspl {quire} as their collective variables, the remaining setup was the same as for firgure \ref  {fig:cgne:naive}, therefore the floating point datatypes show exactly the same values, only posits changed their behavior.}}{19}{figure.7}\protected@file@percent }
\newlabel{fig:cgne:quire}{{7}{19}{In these plots, the posits were utilizing \glspl {quire} as their collective variables, the remaining setup was the same as for firgure \ref {fig:cgne:naive}, therefore the floating point datatypes show exactly the same values, only posits changed their behavior}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The 6 plots introduce a slightly smarter replacement. All collective variables such as norms where calculated in \gls {binary64}, such that a datatype with a small number range such as \gls {binary16} may not over- or underflow when calculating the norm of a vector full of said datatype. This replacement resembles the \gls {quire} for posits. Using this replacement, even heavily reduced datatypes like \gls {binary16} and \gls {posit16} converged and threw a result of equal quality as the one simulated with \gls {binary64}.}}{20}{figure.8}\protected@file@percent }
\newlabel{fig:cgne:col64}{{8}{20}{The 6 plots introduce a slightly smarter replacement. All collective variables such as norms where calculated in \gls {binary64}, such that a datatype with a small number range such as \gls {binary16} may not over- or underflow when calculating the norm of a vector full of said datatype. This replacement resembles the \gls {quire} for posits. Using this replacement, even heavily reduced datatypes like \gls {binary16} and \gls {posit16} converged and threw a result of equal quality as the one simulated with \gls {binary64}}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The configuration in this series of plots is equal to Figure \ref  {fig:cgne:col64}, besides the value of \texttt  {res} - the desired relative residue of the calculated solution - is set to $10^{-12}$ instead of $10^{-6}$. Notice that $10^{-12}$ is outside the representable number range of the datatypes that did not converge; \gls {binary16}, \gls {posit16} and \gls {posit8}.}}{20}{figure.9}\protected@file@percent }
\newlabel{fig:cgne:res12}{{9}{20}{The configuration in this series of plots is equal to Figure \ref {fig:cgne:col64}, besides the value of \code {res} - the desired relative residue of the calculated solution - is set to $10^{-12}$ instead of $10^{-6}$. Notice that $10^{-12}$ is outside the representable number range of the datatypes that did not converge; \gls {binary16}, \gls {posit16} and \gls {posit8}}{figure.9}{}}
\citation{posit2018standard}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.2}$8^4$ lattice}{23}{subsubsection.8.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.3}Conclusion}{23}{subsubsection.8.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces In anology to figures \ref  {fig:cgne:col64} and \ref  {fig:cgne:res12}. This time an $8^4$ lattice was used and only the floating point datatypes that are available in hardware nowadays were simulated. The \textit  {first and second row} use \gls {binary64} as collective variable and $10^{-6}$ was the desired relative residual. The \textit  {third and fourth row} have the exact same setup, but with a relative residual of $10^{-12}$ instead.}}{24}{figure.10}\protected@file@percent }
\newlabel{fig:cgne8}{{10}{24}{In anology to figures \ref {fig:cgne:col64} and \ref {fig:cgne:res12}. This time an $8^4$ lattice was used and only the floating point datatypes that are available in hardware nowadays were simulated. The \textit {first and second row} use \gls {binary64} as collective variable and $10^{-6}$ was the desired relative residual. The \textit {third and fourth row} have the exact same setup, but with a relative residual of $10^{-12}$ instead}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}SAP preconditioned GCR algorithm}{26}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Even-Odd Preconditioning}{26}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Schwarz Alternating Procedure}{27}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Generalized Conjugate Residual algorithm}{27}{subsection.9.3}\protected@file@percent }
\newlabel{eq:fp_gcr}{{{9.1}}{27}{Generalized Conjugate Residual algorithm}{AMS.9}{}}
\newlabel{df:gcr}{{9.1}{28}{Generalized Conjugate Residuals Method}{definition.9.1}{}}
\newlabel{eq:gcr:step}{{9.2}{28}{Generalized Conjugate Residuals Method}{equation.9.2}{}}
\citation{luscher2004}
\citation{luscher2004}
\citation{openqxd}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Pseudo-code for the GCR recursion.}}{29}{algocf.1}\protected@file@percent }
\newlabel{alg:gcr}{{1}{29}{GCR in openQxD}{algocf.1}{}}
\newlabel{eq:gcr:alpha}{{9.3}{29}{Generalized Conjugate Residuals Method}{equation.9.3}{}}
\newlabel{eq:gcr:Api}{{9.5}{29}{}{equation.9.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}GCR in openQxD}{29}{subsection.9.4}\protected@file@percent }
\newlabel{eq:gcr:step:paper}{{9.6}{29}{GCR in openQxD}{equation.9.6}{}}
\newlabel{eq:gcr:Api:equiv}{{9.7}{30}{GCR in openQxD}{equation.9.7}{}}
\newlabel{eq:gcr:paper:rho1}{{9.8}{30}{GCR in openQxD}{equation.9.8}{}}
\newlabel{eq:gcr:paper:rho2}{{{9.9}}{30}{GCR in openQxD}{AMS.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Deflated SAP preconditioned GCR algorithm}{30}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Summary}{30}{section.11}\protected@file@percent }
\bibdata{include/references}
\bibcite{fp24}{1}
\bibcite{openqxd}{2}
\bibcite{cody1980}{3}
\bibcite{posit2018standard}{4}
\bibcite{gustafson2017posit}{5}
\bibcite{gustafson2017}{6}
\bibcite{tf32}{7}
\bibcite{luscher2004}{8}
\bibcite{ieee754_1985}{9}
\bibcite{ieee754_2008}{10}
\bibcite{github}{11}
\bibcite{shewchuk1994}{12}
\bibcite{bfloat16}{13}
\bibstyle{abbrv}
\@writefile{toc}{\contentsline {section}{\numberline {12}References}{31}{section.12}\protected@file@percent }
\citation{github}
\citation{bfloat16}
\citation{ieee754_2008}
\citation{ieee754_2008}
\citation{ieee754_2008}
\citation{posit2018standard}
\citation{posit2018standard}
\citation{posit2018standard}
\citation{posit2018standard}
\citation{posit2018standard}
\citation{gustafson2017posit}
\@writefile{toc}{\contentsline {section}{Appendices}{32}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Code}{32}{subsection.Alph0.1}\protected@file@percent }
\newlabel{sec:code}{{A}{32}{Code}{subsection.Alph0.1}{}}
\@writefile{toc}{\contentsline {section}{Acronyms}{32}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Glossary}{32}{section*.15}\protected@file@percent }
\citation{tf32}
